{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"},{"sourceId":2468672,"sourceType":"datasetVersion","datasetId":1455358},{"sourceId":3824175,"sourceType":"datasetVersion","datasetId":2277617},{"sourceId":4620664,"sourceType":"datasetVersion","datasetId":2663421},{"sourceId":6249263,"sourceType":"datasetVersion","datasetId":3591242},{"sourceId":6851361,"sourceType":"datasetVersion","datasetId":3938197},{"sourceId":12762469,"sourceType":"datasetVersion","datasetId":8067935},{"sourceId":166236,"sourceType":"modelInstanceVersion","modelInstanceId":141449,"modelId":164048},{"sourceId":171496,"sourceType":"modelInstanceVersion","modelInstanceId":145960,"modelId":164048},{"sourceId":182534,"sourceType":"modelInstanceVersion","modelInstanceId":155593,"modelId":178059},{"sourceId":426330,"sourceType":"modelInstanceVersion","modelInstanceId":347541,"modelId":368803},{"sourceId":521642,"sourceType":"modelInstanceVersion","modelInstanceId":410134,"modelId":222398},{"sourceId":523492,"sourceType":"modelInstanceVersion","modelInstanceId":411182,"modelId":429004}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile utils.py\nimport pandas as pd\nimport re\n\n# from rule_augmenter import build_synthetic_rule_map\n\ndef url_to_semantics(text: str) -> str:\n    if not isinstance(text, str):\n        return \"\"\n\n    url_pattern = r'https?://[^\\s/$.?#].[^\\s]*'\n    urls = re.findall(url_pattern, text)\n    \n    if not urls:\n        return \"\" \n\n    all_semantics = []\n    seen_semantics = set()\n\n    for url in urls:\n        url_lower = url.lower()\n        \n        domain_match = re.search(r\"(?:https?://)?([a-z0-9\\-\\.]+)\\.[a-z]{2,}\", url_lower)\n        if domain_match:\n            full_domain = domain_match.group(1)\n            parts = full_domain.split('.')\n            for part in parts:\n                if part and part not in seen_semantics and len(part) > 3: # Avoid short parts like 'www'\n                    all_semantics.append(f\"domain:{part}\")\n                    seen_semantics.add(part)\n\n        # 2. Extract path parts\n        path = re.sub(r\"^(?:https?://)?[a-z0-9\\.-]+\\.[a-z]{2,}/?\", \"\", url_lower)\n        path_parts = [p for p in re.split(r'[/_.-]+', path) if p and p.isalnum()] # Split by common delimiters\n\n        for part in path_parts:\n            # Clean up potential file extensions or query params\n            part_clean = re.sub(r\"\\.(html?|php|asp|jsp)$|#.*|\\?.*\", \"\", part)\n            if part_clean and part_clean not in seen_semantics and len(part_clean) > 3:\n                all_semantics.append(f\"path:{part_clean}\")\n                seen_semantics.add(part_clean)\n\n    if not all_semantics:\n        return \"\"\n\n    return f\"\\nURL Keywords: {' '.join(all_semantics)}\"\n\n\ndef get_dataframe_to_train(data_path):\n    train_dataset = pd.read_csv(f\"{data_path}/train.csv\") \n    test_dataset = pd.read_csv(f\"{data_path}/test.csv\")\n\n    flatten = []\n\n    flatten.append(train_dataset[[\"body\", \"rule\", \"subreddit\",\"rule_violation\"]].copy())\n\n    for violation_type in [\"positive\", \"negative\"]:\n        for i in range(1, 3):\n            col_name = f\"{violation_type}_example_{i}\"\n            \n            if col_name in train_dataset.columns:\n                sub_dataset = train_dataset[[col_name, \"rule\", \"subreddit\"]].copy()\n                sub_dataset = sub_dataset.rename(columns={col_name: \"body\"})\n                sub_dataset[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n                \n                sub_dataset.dropna(subset=['body'], inplace=True)\n                sub_dataset = sub_dataset[sub_dataset['body'].str.strip().str.len() > 0]\n                \n                if not sub_dataset.empty:\n                    flatten.append(sub_dataset)\n    \n    for violation_type in [\"positive\", \"negative\"]:\n        for i in range(1, 3):\n            col_name = f\"{violation_type}_example_{i}\"\n            \n            if col_name in test_dataset.columns:\n                sub_dataset = test_dataset[[col_name, \"rule\", \"subreddit\"]].copy()\n                sub_dataset = sub_dataset.rename(columns={col_name: \"body\"})\n                sub_dataset[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n                \n                sub_dataset.dropna(subset=['body'], inplace=True)\n                sub_dataset = sub_dataset[sub_dataset['body'].str.strip().str.len() > 0]\n                \n                if not sub_dataset.empty:\n                    flatten.append(sub_dataset)\n    \n    dataframe = pd.concat(flatten, axis=0)\n    dataframe = dataframe.drop_duplicates(subset=['body', 'rule', 'subreddit'], ignore_index=True)\n    dataframe.drop_duplicates(subset=['body','rule'],keep='first',inplace=True)\n    \n    return dataframe.sample(frac=1, random_state=42).reset_index(drop=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-23T03:24:54.529947Z","iopub.execute_input":"2025-10-23T03:24:54.530255Z","iopub.status.idle":"2025-10-23T03:24:54.539178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train_albert.py\nimport os\nimport pandas as pd\nimport torch\nimport random\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments\n)\nfrom utils import get_dataframe_to_train, url_to_semantics\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nclass CFG:\n    model_name_or_path = \"/kaggle/input/transformers/albert-large-v2\"  # Sử dụng ALBERT-large-v2\n    data_path = \"/kaggle/input/jigsaw-agile-community-rules/\"\n    output_dir = \"./albert_final_model\"\n    EPOCHS = 3\n    LEARNING_RATE = 2e-5\n    MAX_LENGTH = 512\n    BATCH_SIZE = 8  # Phù hợp với Tesla T4, có thể thử 12 nếu GPU memory đủ\n\nclass JigsawDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels:\n            item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\ndef main():\n    seed_everything(42)\n    training_data_df = get_dataframe_to_train(CFG.data_path)\n    print(f\"Training dataset size: {len(training_data_df)}\")\n\n    test_df_for_prediction = pd.read_csv(f\"{CFG.data_path}/test.csv\")\n    \n    # Preprocessing: Thêm URL semantics và kết hợp rule + body\n    training_data_df['body_with_url'] = training_data_df['body'].apply(lambda x: x + url_to_semantics(x))\n    training_data_df['input_text'] = training_data_df['rule'] + \"[SEP]\" + training_data_df['body_with_url']\n\n    # Tokenizer và encodings\n    tokenizer = AutoTokenizer.from_pretrained(CFG.model_name_or_path)\n    train_encodings = tokenizer(training_data_df['input_text'].tolist(), truncation=True, padding=True, max_length=CFG.MAX_LENGTH)\n    train_labels = training_data_df['rule_violation'].tolist()\n    train_dataset = JigsawDataset(train_encodings, train_labels)\n\n    # Load model\n    model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name_or_path, num_labels=2)\n    \n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=CFG.output_dir,\n        num_train_epochs=CFG.EPOCHS,\n        learning_rate=CFG.LEARNING_RATE,\n        per_device_train_batch_size=CFG.BATCH_SIZE,\n        warmup_ratio=0.1,\n        weight_decay=0.01,\n        report_to=\"none\",\n        save_strategy=\"no\",\n        logging_steps=1,\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n    )\n    \n    trainer.train()\n\n    # Inference trên test set\n    test_df_for_prediction['body_with_url'] = test_df_for_prediction['body'].apply(lambda x: x + url_to_semantics(x))\n    test_df_for_prediction['input_text'] = test_df_for_prediction['rule'] + \"[SEP]\" + test_df_for_prediction['body_with_url']\n    \n    test_encodings = tokenizer(test_df_for_prediction['input_text'].tolist(), truncation=True, padding=True, max_length=CFG.MAX_LENGTH)\n    test_dataset = JigsawDataset(test_encodings)\n    \n    predictions = trainer.predict(test_dataset)\n    probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1)[:, 1].numpy()\n\n    # Lưu submission\n    submission_df = pd.DataFrame({\n        \"row_id\": test_df_for_prediction[\"row_id\"],\n        \"rule_violation\": probs\n    })\n    submission_df.to_csv(\"submission_albert.csv\", index=False)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train_albert.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train_deberta.py\nimport os\nimport pandas as pd\nimport torch\nimport random\nimport numpy as np\nfrom sklearn.model_selection import train_test_split \nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments\n)\n\nfrom utils import get_dataframe_to_train, url_to_semantics\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nclass CFG:\n    model_name_or_path = \"/kaggle/input/huggingfacedebertav3variants/mdeberta-v3-base\"\n    data_path = \"/kaggle/input/jigsaw-agile-community-rules/\"\n    output_dir = \"./deberta_v3_small_final_model\"\n  \n    EPOCHS = 3\n    LEARNING_RATE = 2e-5  \n    \n    MAX_LENGTH = 512\n    BATCH_SIZE = 8\n\nclass JigsawDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels:\n            item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\ndef main():\n    seed_everything(42)\n    training_data_df = get_dataframe_to_train(CFG.data_path)\n    # training_data_df, valid_df = train_test_split(full_df,test_size=0.2,stratify=full_df['rule'],random_state=42)\n    print(f\"Training dataset (from examples only) size: {len(training_data_df)}\")\n\n    test_df_for_prediction = pd.read_csv(f\"{CFG.data_path}/test.csv\")\n    \n    training_data_df['body_with_url'] = training_data_df['body'].apply(lambda x: x + url_to_semantics(x))\n    training_data_df['input_text'] = training_data_df['rule'] + \"[SEP]\" + training_data_df['body_with_url']\n\n    tokenizer = AutoTokenizer.from_pretrained(CFG.model_name_or_path)\n    train_encodings = tokenizer(training_data_df['input_text'].tolist(), truncation=True, padding=True, max_length=CFG.MAX_LENGTH)\n    train_labels = training_data_df['rule_violation'].tolist()\n    train_dataset = JigsawDataset(train_encodings, train_labels)\n\n    model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name_or_path, num_labels=2)\n    \n    training_args = TrainingArguments(\n        output_dir=CFG.output_dir,\n        num_train_epochs=CFG.EPOCHS,\n        learning_rate=CFG.LEARNING_RATE,\n        per_device_train_batch_size=CFG.BATCH_SIZE,\n        warmup_ratio=0.1,\n        weight_decay=0.01,\n        report_to=\"none\",\n        save_strategy=\"no\",  #这一行加上这个 save_strategy=\"no\"\n        logging_steps=1,\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n    )\n    \n    trainer.train()\n\n    test_df_for_prediction['body_with_url'] = test_df_for_prediction['body'].apply(lambda x: x + url_to_semantics(x))\n    test_df_for_prediction['input_text'] = test_df_for_prediction['rule'] + \"[SEP]\" + test_df_for_prediction['body_with_url']\n    \n    test_encodings = tokenizer(test_df_for_prediction['input_text'].tolist(), truncation=True, padding=True, max_length=CFG.MAX_LENGTH)\n    test_dataset = JigsawDataset(test_encodings)\n    \n    predictions = trainer.predict(test_dataset)\n    probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1)[:, 1].numpy()\n\n    submission_df = pd.DataFrame({\n        \"row_id\": test_df_for_prediction[\"row_id\"],\n        \"rule_violation\": probs\n    })\n    submission_df.to_csv(\"submission_deberta.csv\", index=False)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train_deberta.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train_distilroberta.py\nimport os\nimport pandas as pd\nimport torch\nimport random\nimport numpy as np\nfrom sklearn.model_selection import train_test_split \nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments\n)\n\nfrom utils import get_dataframe_to_train, url_to_semantics\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nclass CFG:\n    model_name_or_path = \"/kaggle/input/distilroberta-base/distilroberta-base\"\n    data_path = \"/kaggle/input/jigsaw-agile-community-rules/\"\n    output_dir = \"./deberta_v3_small_final_model\"\n  \n    EPOCHS = 3\n    LEARNING_RATE = 2e-5  \n    \n    MAX_LENGTH = 512\n    BATCH_SIZE = 8\n\nclass JigsawDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels:\n            item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\ndef main():\n    seed_everything(42)\n    training_data_df = get_dataframe_to_train(CFG.data_path)\n    # training_data_df, valid_df = train_test_split(full_df,test_size=0.2,stratify=full_df['rule'],random_state=42)\n    print(f\"Training dataset (from examples only) size: {len(training_data_df)}\")\n\n    test_df_for_prediction = pd.read_csv(f\"{CFG.data_path}/test.csv\")\n    \n    training_data_df['body_with_url'] = training_data_df['body'].apply(lambda x: x + url_to_semantics(x))\n    training_data_df['input_text'] = training_data_df['rule'] + \"[SEP]\" + training_data_df['body_with_url']\n\n    tokenizer = AutoTokenizer.from_pretrained(CFG.model_name_or_path)\n    train_encodings = tokenizer(training_data_df['input_text'].tolist(), truncation=True, padding=True, max_length=CFG.MAX_LENGTH)\n    train_labels = training_data_df['rule_violation'].tolist()\n    train_dataset = JigsawDataset(train_encodings, train_labels)\n\n    model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name_or_path, num_labels=2)\n    \n    training_args = TrainingArguments(\n        output_dir=CFG.output_dir,\n        num_train_epochs=CFG.EPOCHS,\n        learning_rate=CFG.LEARNING_RATE,\n        per_device_train_batch_size=CFG.BATCH_SIZE,\n        warmup_ratio=0.1,\n        weight_decay=0.01,\n        report_to=\"none\",\n        save_strategy=\"no\",  #这一行加上这个 save_strategy=\"no\"\n        logging_steps=1,\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n    )\n    \n    trainer.train()\n\n    test_df_for_prediction['body_with_url'] = test_df_for_prediction['body'].apply(lambda x: x + url_to_semantics(x))\n    test_df_for_prediction['input_text'] = test_df_for_prediction['rule'] + \"[SEP]\" + test_df_for_prediction['body_with_url']\n    \n    test_encodings = tokenizer(test_df_for_prediction['input_text'].tolist(), truncation=True, padding=True, max_length=CFG.MAX_LENGTH)\n    test_dataset = JigsawDataset(test_encodings)\n    \n    predictions = trainer.predict(test_dataset)\n    probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1)[:, 1].numpy()\n\n    submission_df = pd.DataFrame({\n        \"row_id\": test_df_for_prediction[\"row_id\"],\n        \"rule_violation\": probs\n    })\n    submission_df.to_csv(\"submission_distilroberta.csv\", index=False)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train_distilroberta.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train_deberta_auc.py\nimport os\nimport pandas as pd\nimport torch\nimport random\nimport numpy as np\nfrom sklearn.model_selection import train_test_split \nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments\n)\n\nfrom utils import get_dataframe_to_train, url_to_semantics\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nclass CFG:\n    model_name_or_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-base\"\n    data_path = \"/kaggle/input/jigsaw-agile-community-rules/\"\n    output_dir = \"./deberta_v3_small_final_model\"\n  \n    EPOCHS = 3\n    LEARNING_RATE = 4e-5  \n    \n    MAX_LENGTH = 512\n    BATCH_SIZE = 12\n\nclass JigsawDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels:\n            item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\ndef main():\n    seed_everything(42)\n    training_data_df = get_dataframe_to_train(CFG.data_path)\n    # training_data_df, valid_df = train_test_split(full_df,test_size=0.2,stratify=full_df['rule'],random_state=42)\n    print(f\"Training dataset (from examples only) size: {len(training_data_df)}\")\n\n    test_df_for_prediction = pd.read_csv(f\"{CFG.data_path}/test.csv\")\n    \n    training_data_df['body_with_url'] = training_data_df['body'].apply(lambda x: x + url_to_semantics(x))\n    training_data_df['input_text'] = training_data_df['rule'] + \"[SEP]\" + training_data_df['body_with_url']\n\n    tokenizer = AutoTokenizer.from_pretrained(CFG.model_name_or_path)\n    train_encodings = tokenizer(training_data_df['input_text'].tolist(), truncation=True, padding=True, max_length=CFG.MAX_LENGTH)\n    train_labels = training_data_df['rule_violation'].tolist()\n    train_dataset = JigsawDataset(train_encodings, train_labels)\n\n    model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name_or_path, num_labels=2)\n    \n    training_args = TrainingArguments(\n        output_dir=CFG.output_dir,\n        num_train_epochs=CFG.EPOCHS,\n        learning_rate=CFG.LEARNING_RATE,\n        per_device_train_batch_size=CFG.BATCH_SIZE,\n        warmup_ratio=0.1,\n        weight_decay=0.01,\n        report_to=\"none\",\n        save_strategy=\"no\",  #这一行加上这个 save_strategy=\"no\"\n        logging_steps=1,\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n    )\n    \n    trainer.train()\n\n    test_df_for_prediction['body_with_url'] = test_df_for_prediction['body'].apply(lambda x: x + url_to_semantics(x))\n    test_df_for_prediction['input_text'] = test_df_for_prediction['rule'] + \"[SEP]\" + test_df_for_prediction['body_with_url']\n    \n    test_encodings = tokenizer(test_df_for_prediction['input_text'].tolist(), truncation=True, padding=True, max_length=CFG.MAX_LENGTH)\n    test_dataset = JigsawDataset(test_encodings)\n    \n    predictions = trainer.predict(test_dataset)\n    probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1)[:, 1].numpy()\n\n    submission_df = pd.DataFrame({\n        \"row_id\": test_df_for_prediction[\"row_id\"],\n        \"rule_violation\": probs\n    })\n    submission_df.to_csv(\"submission_debertaauc.csv\", index=False)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train_deberta_auc.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile constants.py\nEMBDEDDING_MODEL_PATH = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"\nMODEL_OUTPUT_PATH = '/kaggle/input/qwen3-8b-embedding'\nDATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules\"\n\n# https://huggingface.co/Qwen/Qwen3-Embedding-0.6B/blob/main/config_sentence_transformers.json\nEMBEDDING_MODEL_QUERY = \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery:\"\n\nCLEAN_TEXT = True\nTOP_K = 2000\nBATCH_SIZE = 128","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile utils.py\nimport pandas as pd\nimport torch.distributed as dist\n\nfrom datasets import Dataset\nfrom cleantext import clean\nfrom tqdm.auto import tqdm\n\nfrom constants import CLEAN_TEXT\n\n\ndef build_prompt(row):\n    return f\"\"\"r/{row[\"subreddit\"]}\\nComment: {row[\"body\"]}\"\"\"\n\n\ndef cleaner(text):\n    return clean(\n        text,\n        fix_unicode=True,\n        to_ascii=True,\n        lower=False,\n        no_line_breaks=False,\n        no_urls=True,\n        no_emails=True,\n        no_phone_numbers=True,\n        no_numbers=False,\n        no_digits=False,\n        no_currency_symbols=False,\n        no_punct=False,\n        replace_with_url=\"<URL>\",\n        replace_with_email=\"<EMAIL>\",\n        replace_with_phone_number=\"<PHONE>\",\n        lang=\"en\",\n    )\n\n\n\ndef get_dataframe_to_train(data_path):\n    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.6, random_state=42).reset_index(drop=True)\n\n    flatten = []\n    flatten.append(train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\"]])\n    \n    for violation_type in [\"positive\", \"negative\"]:\n        for i in range(1, 3):\n            sub_dataset = test_dataset[[f\"{violation_type}_example_{i}\", \"rule\", \"subreddit\"]].copy()\n            sub_dataset = sub_dataset.rename(columns={f\"{violation_type}_example_{i}\": \"body\"})\n            sub_dataset[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n            flatten.append(sub_dataset)\n\n    dataframe = pd.concat(flatten, axis=0)    \n    dataframe = dataframe.drop_duplicates(ignore_index=True)\n    return dataframe\n\n\ndef prepare_dataframe(dataframe):\n    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n\n    \n    if CLEAN_TEXT:\n        tqdm.pandas(desc=\"cleaner\")\n        dataframe[\"prompt\"] = dataframe[\"prompt\"].progress_apply(cleaner)\n\n    if \"rule_violation\" in dataframe.columns:\n        dataframe[\"rule_violation\"] = dataframe[\"rule_violation\"].map(\n            {\n                1: 1,\n                0: -1,\n            }\n        )\n\n    return dataframe","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile semantic.py\nimport pandas as pd\nfrom transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import semantic_search, dot_score\nfrom tqdm.auto import tqdm\nfrom peft import PeftModel, PeftConfig\n\n\nfrom utils import get_dataframe_to_train, prepare_dataframe\nfrom constants import DATA_PATH, EMBDEDDING_MODEL_PATH, EMBEDDING_MODEL_QUERY, TOP_K, BATCH_SIZE, MODEL_OUTPUT_PATH\n\n\n\ndef get_scores(test_dataframe):\n    corpus_dataframe = get_dataframe_to_train(DATA_PATH)\n    corpus_dataframe = prepare_dataframe(corpus_dataframe)\n    \n    # Load base model\n    model = AutoModelForCausalLM.from_pretrained(EMBDEDDING_MODEL_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(EMBDEDDING_MODEL_PATH)\n    \n    # Load adapter configuration and model\n    adapter_config = PeftConfig.from_pretrained(MODEL_OUTPUT_PATH)\n    lora_model = PeftModel.from_pretrained(model, MODEL_OUTPUT_PATH, config=adapter_config)\n    merged_model = lora_model.merge_and_unload()\n    tokenizer.save_pretrained(\"Qwen3Emb_Finetuned\")\n    merged_model.save_pretrained(\"Qwen3Emb_Finetuned\")\n\n    # 4. Tạo lại SentenceTransformer từ encoder đã merge\n    embedding_model = SentenceTransformer(model_name_or_path=\"Qwen3Emb_Finetuned\", device=\"cuda\")\n\n    print('Done loading model!')\n\n    result = []\n    for rule in tqdm(test_dataframe[\"rule\"].unique(), desc=f\"Generate scores for each rule\"):\n        test_dataframe_part = test_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n        corpus_dataframe_part = corpus_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n        corpus_dataframe_part = corpus_dataframe_part.reset_index(names=\"row_id\")\n        \n        query_embeddings = embedding_model.encode(\n            sentences=test_dataframe_part[\"prompt\"].tolist(),\n            prompt=EMBEDDING_MODEL_QUERY,\n            batch_size=BATCH_SIZE,\n            show_progress_bar=True,\n            convert_to_tensor=True,\n            device=\"cuda\",\n            normalize_embeddings=True,\n        )\n        document_embeddings = embedding_model.encode(\n            sentences=corpus_dataframe_part[\"prompt\"].tolist(),\n            batch_size=BATCH_SIZE,\n            show_progress_bar=True,\n            convert_to_tensor=True,\n            device=\"cuda\",\n            normalize_embeddings=True,\n        )\n        test_dataframe_part[\"semantic\"] = semantic_search(\n            query_embeddings,\n            document_embeddings,\n            top_k=TOP_K,\n            score_function=dot_score,\n        )\n        def get_score(semantic):\n            semantic = pd.DataFrame(semantic)\n            semantic = semantic.merge(\n                corpus_dataframe_part[[\"row_id\", \"rule_violation\"]],\n                how=\"left\",\n                left_on=\"corpus_id\",\n                right_on=\"row_id\",\n            )\n            semantic[\"score\"] = semantic[\"score\"]*semantic[\"rule_violation\"]\n            return semantic[\"score\"].sum()\n            \n        tqdm.pandas(desc=f\"Add label for {rule=}\")\n        test_dataframe_part[\"rule_violation\"] = test_dataframe_part[\"semantic\"].progress_apply(get_score)\n        result.append(test_dataframe_part[[\"row_id\", \"rule_violation\"]].copy())\n        \n    submission = pd.concat(result, axis=0)\n    return submission\n\n\ndef generate_submission():\n    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n    test_dataframe = prepare_dataframe(test_dataframe)\n    \n    submission = get_scores(test_dataframe)\n    submission = test_dataframe[[\"row_id\"]].merge(submission, on=\"row_id\", how=\"left\")\n    submission.to_csv(\"submission_qwen3.csv\", index=False)\n\n\nif __name__ == \"__main__\":\n    generate_submission()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python semantic.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile infer_qwen.py\n\nimport os\nimport pandas as pd\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\nimport torch\nimport vllm\nimport numpy as np\nfrom vllm.lora.request import LoRARequest\nimport argparse\nfrom scipy.special import softmax\ndf = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\")\n\nMODEL_NAME = \"/kaggle/input/qwen2.5/transformers/14b-instruct-gptq-int4/1\"\n#MODEL_NAME = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\nLORA_PATH = \"/kaggle/input/lora_14b_gptq_1epoch_r32/keras/default/1\"\nif __name__=='__main__':\n    os.environ[\"VLLM_USE_V1\"] = \"0\"\n\n    llm = vllm.LLM(\n        MODEL_NAME,\n        # quantization='awq',\n        quantization='gptq',\n        tensor_parallel_size=torch.cuda.device_count(),\n        gpu_memory_utilization=0.98,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=2836,\n        disable_log_stats=True,\n        enable_prefix_caching=True,\n        enable_lora=True,\n        max_lora_rank=32\n    )\n    tokenizer = llm.get_tokenizer()\n    SYS_PROMPT = \"\"\"\nYou are given a comment on reddit. Your task is to classify if it violates the given rule. Only respond Yes/No.\n\"\"\"\n    \n    prompts = []\n    for i, row in df.iterrows():\n        text = f\"\"\"\n    r/{row.subreddit}\n    Rule: {row.rule}\n    \n    1) {row.positive_example_1}\n    Violation: Yes\n    \n    2) {row.positive_example_2}\n    Violation: Yes\n    \n    3) {row.negative_example_1}\n    Violation: No\n    \n    4) {row.negative_example_2}\n    Violation: No\n    \n    5) {row.body}\n    \"\"\"\n        \n        messages = [\n            {\"role\": \"system\", \"content\": SYS_PROMPT},\n            {\"role\": \"user\", \"content\": text}\n        ]\n    \n        prompt = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=False,\n        ) + \"Answer:\"\n        prompts.append(prompt)\n    \n    df[\"prompt\"] = prompts\n    \n    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=['Yes','No'])\n    outputs = llm.generate(\n        prompts,\n        vllm.SamplingParams(\n            skip_special_tokens=True,\n            max_tokens=1,\n            logits_processors=[mclp],\n            logprobs=2,\n        ),\n        use_tqdm=True,\n        lora_request=LoRARequest(\"default\", 1, LORA_PATH)\n    )\n    logprobs = [\n        {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n        for out in outputs\n    ]\n    logit_matrix = pd.DataFrame(logprobs)[['Yes','No']]\n    df = pd.concat([df, logit_matrix], axis=1)\n    \n    df[['Yes',\"No\"]] = df[['Yes',\"No\"]].apply(lambda x: softmax(x.values), axis=1, result_type=\"expand\")\n    df[\"pred\"] = df[\"Yes\"]\n    df['rule_violation'] = df[\"pred\"]\n    df[['row_id', 'rule_violation']].to_csv(\"submission_qwen14b.csv\",index=False)\n    pd.read_csv('submission_qwen14b.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python infer_qwen.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nq = pd.read_csv('submission_deberta.csv')\nl = pd.read_csv('submission_qwen3.csv')\nm = pd.read_csv('submission_distilroberta.csv')\nw = pd.read_csv('submission_qwen14b.csv')\na = pd.read_csv('submission_debertaauc.csv')\nal= pd.read_csv('submission_albert.csv')\nsq= pd.read_csv('submission_squeezebert.csv')\n\nrq = q['rule_violation'].rank(method='average') / (len(q)+1)\nrl = l['rule_violation'].rank(method='average') / (len(l)+1)\nrm = m['rule_violation'].rank(method='average') / (len(m)+1)\nrw = w['rule_violation'].rank(method='average') / (len(w)+1)\nra = a['rule_violation'].rank(method='average') / (len(a)+1)\nral = al['rule_violation'].rank(method='average') / (len(al)+1)\nrsq = sq['rule_violation'].rank(method='average') / (len(sq)+1)\n\nblend = 0.25*rq + 0.15*rl + 0.15*rm + 0.15*rw + 0.10*ra + 0.10*ral + 0.05*rsq\n\nq['rule_violation'] = blend\nq.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\npd.read_csv('/kaggle/working/submission.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Done","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}